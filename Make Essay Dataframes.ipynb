{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read in the ann files of the essay corpus and transform them into dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from brat_parser import get_entities_relations_attributes_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##read in ANN files\n",
    "ent_list=[]\n",
    "rel_list=[]\n",
    "att_list=[]\n",
    "for i in range(1,103):\n",
    "    \n",
    "    try:\n",
    "        if len(str(i))==1:\n",
    "            string=\"00\"+str(i)\n",
    "        elif len(str(i))==2:\n",
    "                string=\"0\"+str(i)\n",
    "        elif len(str(i))==3:\n",
    "            string=str(i)\n",
    "        else:\n",
    "            raise ValueError\n",
    "    except ValueError:\n",
    "        print(\"The number\"+str(i)+\"is not expected. The number has to be between 0 and 999.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "   #input is split in the files between different annotations\n",
    "    entities, relations, attributes, groups = get_entities_relations_attributes_groups(\"essays_all\\essay\"+string+\".ann\")\n",
    "    \n",
    "    ent_list.append(entities)\n",
    "    rel_list.append(relations)\n",
    "    att_list.append(attributes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relations_frame(relations):\n",
    "    rel=pd.DataFrame(relations.items())\n",
    "    rel=rel.drop([0],axis=1)\n",
    "\n",
    "    x = rel.to_string(header=False,\n",
    "                              index=False,\n",
    "                              index_names=False).split('\\n')\n",
    "\n",
    "    rel_frame=pd.DataFrame([sub.split(\",\") for sub in x[:]])\n",
    "\n",
    "\n",
    "    for i in range(len(rel_frame)):\n",
    "\n",
    "        #simplify the strings by removing superflous information\n",
    "        rel_frame.iloc[i,0]=re.search(\"'(.*)'\",rel_frame.iloc[i,0]).group(1)\n",
    "\n",
    "        rel_frame.iloc[i,1]=re.search(\"'(.*)'\",rel_frame.iloc[i,1]).group(1)\n",
    "\n",
    "        rel_frame.iloc[i,2]=re.search(\"'(.*)'\",rel_frame.iloc[i,2]).group(1)\n",
    "\n",
    "        rel_frame.iloc[i,3]=re.search(\"'(.*)'\",rel_frame.iloc[i,3]).group(1)\n",
    "\n",
    "    rel_frame=rel_frame.rename(columns={0: \"ID\", 1: \"Stance\", 2:\"Targets\", 3:\"Supported\"})\n",
    "\n",
    "    return rel_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the entities frame\n",
    "#Returns a Dataframe with the text the Target ID and the type of the Target (Claim,Prem,MJClaim)\n",
    "def get_entities_frame(entities):\n",
    "    \n",
    "    ent=pd.DataFrame(entities.items())\n",
    "    ent=ent.drop([0],axis=1)\n",
    "    \n",
    "    #split information into columns\n",
    "    x = ent.to_string(header=False,\n",
    "                          index=False,\n",
    "                          index_names=False).split('\\n')\n",
    "\n",
    "    ent_frame=pd.DataFrame([sub.split(\",\") for sub in x[:]])\n",
    "    \n",
    "    #since the split happens along \",\" the text might be split over multiple columns\n",
    "    #combine them again into one column\n",
    "    ent_frame['text'] = ent_frame[ent_frame.columns[5:]].apply(\n",
    "    lambda x: ','.join(x.dropna().astype(str)),\n",
    "    axis=1\n",
    "    )\n",
    "    \n",
    "    #move text column to front and drop a columns with text fragments form the back of the frame\n",
    "    cols = ent_frame.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    ent_frame = ent_frame[cols]\n",
    "\n",
    "    ent_frame=ent_frame.drop(ent_frame.columns[3:],axis=1)\n",
    "\n",
    "    #clean up text\n",
    "    for i in range(len(ent_frame)):\n",
    "        #ent_frame.iloc[i,0]=re.search(\"text='(.*)'\",ent_frame.iloc[i,0]).group(1)\n",
    "        ent_frame.iloc[i,1]=re.search(\"'(.*)'\",ent_frame.iloc[i,1]).group(1)\n",
    "        ent_frame.iloc[i,2]=re.search(\"'(.*)'\",ent_frame.iloc[i,2]).group(1)\n",
    "\n",
    "\n",
    "    ent_frame=ent_frame.rename(columns={0: \"Targets\", 1: \"Type\"})    \n",
    "\n",
    "    return ent_frame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the attributes into a readable dataframe\n",
    "def get_attribute_frame(attributes):\n",
    "    #Transform the attributes into one dataframe\n",
    "    att=pd.DataFrame(attributes.items())\n",
    "\n",
    "    att=att.drop([0],axis=1)\n",
    "\n",
    "    #get the values into a comma seperated list to make them accessible\n",
    "    x = att.to_string(header=False,\n",
    "                      index=False,\n",
    "                      index_names=False).split('\\n')\n",
    "\n",
    "    vals = [','.join(ele.split()) for ele in x]\n",
    "    \n",
    "    #clean up dataframe\n",
    "    attribute_frame=pd.DataFrame([sub.split(\",\") for sub in vals[:]])\n",
    "\n",
    "    #rename columns\n",
    "    attribute_frame=attribute_frame.rename(columns={0: \"ID\", 2: \"Type\", 4:\"Targets\",6:\"Values\"})\n",
    "\n",
    "    \n",
    "    \n",
    "    for i in range(len(attribute_frame)):\n",
    "\n",
    "        #simplify the strings by removing superflous information\n",
    "        attribute_frame.iloc[i,0]=re.search(\"'(.*)'\",attribute_frame.iloc[i,0]).group(1)\n",
    "\n",
    "        attribute_frame.iloc[i,2]=re.search(\"'(.*)'\",attribute_frame.iloc[i,2]).group(1)\n",
    "\n",
    "        attribute_frame.iloc[i,4]=re.search(\"'(.*)'\",attribute_frame.iloc[i,4]).group(1)\n",
    "\n",
    "        attribute_frame.iloc[i,6]=re.search(\"'(.*)'\",attribute_frame.iloc[i,6]).group(1)\n",
    "\n",
    "    #drop empyt columns\n",
    "    attribute_frame=attribute_frame.drop([1], axis=1)\n",
    "    attribute_frame=attribute_frame.drop([3], axis=1)\n",
    "    attribute_frame=attribute_frame.drop([5], axis=1)\n",
    "    attribute_frame=attribute_frame.drop([7], axis=1)\n",
    "\n",
    "\n",
    "   \n",
    "    return attribute_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert into dataframes each list entry is one text \n",
    "att_frame_list=[]\n",
    "ent_frame_list=[]\n",
    "rel_frame_list=[]\n",
    "\n",
    "for i,j,k in zip(att_list,ent_list,rel_list):\n",
    "    att_frame_list.append(get_attribute_frame(i))\n",
    "    ent_frame_list.append(get_entities_frame(j))\n",
    "    rel_frame_list.append(get_relations_frame(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pickle.dump(att_frame_list, open( \"att_frame_list.p\", \"wb\" ) )\n",
    "#pickle.dump(ent_frame_list, open( \"ent_frame_list.p\", \"wb\" ) )\n",
    "#pickle.dump(rel_frame_list, open( \"rel_frame_list.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "0acc6f63e5576177c66e256d998b6d3964615254c3e54055c5c863c51bc5dfdf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
